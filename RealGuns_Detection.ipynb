{"cells":[{"cell_type":"markdown","metadata":{"id":"-Ls24vSmDsJp"},"source":["## **Owner**: Ahmed Tarek Mohamed"]},{"cell_type":"markdown","metadata":{"id":"P90MGPibD-pB"},"source":["# **Object Recognition for Real Guns and Water Guns.**"]},{"cell_type":"markdown","metadata":{"id":"3-v7KtM18r3L"},"source":["# **1. Import Libraries and Tools**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0EzYPzy0vT3"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n","import glob\n","import cv2\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","from ipywidgets import Text, VBox"]},{"cell_type":"markdown","metadata":{"id":"cFOnNWRZ-_9i"},"source":["**Import Essential libraries and tools.**\n","* Imports the TensorFlow library and aliases it as tf.\n","* Imports specific modules from TensorFlow's Keras API for building neural\n","* network models (Sequential, Dense, Conv2D, MaxPool2D, Flatten).\n","* Imports the glob module for pattern matching file paths.\n","* Imports the OpenCV library for computer vision tasks, including image processing.\n","* Imports the train_test_split function from scikit-learn for splitting data into training and testing sets.\n","* Imports the NumPy library for numerical operations.\n","* Imports the matplotlib.pyplot module for creating visualizations.\n","* Imports the Image module from the Python Imaging Library (PIL) for image processing.\n","* Imports the requests module for making HTTP requests.\n","* Imports the BytesIO class from the io module for handling binary data as file-like objects.\n","* Imports specific widgets (Text and VBox) from the ipywidgets library for interactive user interfaces."]},{"cell_type":"markdown","metadata":{"id":"RBqTLTVg9Z0a"},"source":["# **2. Class Definition for Image Classification**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rwg8Snd8dhx"},"outputs":[],"source":["class ImageClassifier:\n","    def __init__(self, data_path, categories):\n","        self.data_path = data_path\n","        self.categories = categories\n","        self.model = None\n","\n","    def load_images(self):\n","        data = []\n","        for idx, category in enumerate(self.categories):\n","            for file in glob.glob(f\"{self.data_path}/{category}/*.jpg\"):\n","                image = cv2.imread(file)\n","                if image is not None:\n","                    resized_image = cv2.resize(image, (50, 50))  # Change image size\n","                    data.append([resized_image, idx])\n","        return data\n","\n","    def train_model(self, data):\n","        X = np.array([i[0] for i in data], dtype='float32') / 255.0\n","        y = np.array([i[1] for i in data])\n","        X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.2)\n","        X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5)\n","        self.model = Sequential([\n","            Conv2D(16, (3, 3), activation='relu', input_shape=(50, 50, 3)),\n","            MaxPool2D(2, 2),\n","            Conv2D(32, (3, 3), activation='relu'),\n","            MaxPool2D(2, 2),\n","            Flatten(),\n","            Dense(64, activation='relu'),\n","            Dense(len(self.categories), activation='softmax')\n","        ])\n","        self.model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        self.model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n","        loss, accuracy = self.model.evaluate(X_test, y_test)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","\n","    def predict_image(self, image_url):\n","        response = requests.get(image_url)\n","        img = Image.open(BytesIO(response.content))\n","        img = img.resize((50, 50))\n","        img_array = np.expand_dims(np.array(img), axis=0) / 255.0\n","        prediction = self.model.predict(img_array)\n","        predicted_class = np.argmax(prediction)\n","        plt.imshow(img)\n","        plt.title(f\"Predicted: {self.categories[predicted_class]}\")\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3AEv_XXb9R2w"},"source":["**This code defines the ImageClassifier class, which encapsulates all functionalities related to image classification. It includes methods for loading images, training a convolutional neural network (CNN), and predicting the category of a new image. The class is initialized with a data path and a list of image categories.**\n","\n","* The ImageClassifier class is designed for image classification tasks.\n","* The constructor (__init__) initializes the object with the provided data_path and categories.\n","* The load_images method reads and loads image data from the specified data_path for each category.\n","* The images are resized to 50x50 pixels and stored along with their corresponding category indices in the data list.\n","* The train_model method prepares the data for training, splits it into training, validation, and test sets, and creates a Convolutional Neural Network (CNN) model using the Keras Sequential API.\n","* The CNN model consists of convolutional layers with ReLU activation, max-pooling layers, and densely connected layers with softmax activation for classification.\n","* The model is compiled with the RMSprop optimizer and sparse categorical crossentropy loss function.\n","* It is trained on the training data for 20 epochs, and its performance is evaluated on the validation set.\n","* The trained model is stored in the self.model attribute.\n","* The predict_image method takes an image URL, downloads the image, resizes it to 50x50 pixels, and preprocesses it for prediction.\n","* The trained model predicts the class probabilities for the input image, and the predicted class is displayed along with the image using Matplotlib."]},{"cell_type":"markdown","metadata":{"id":"Rmsbp_NQ-FDo"},"source":["# **3. Main Execution ***"]},{"cell_type":"markdown","metadata":{"id":"Wwy7DzjPG7O2"},"source":["### TO TEST THE CODE USE IMAGE URL OF OBJECTS SIMILAR TO GUN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":778,"referenced_widgets":["54b62f6d7159459c8e3bc3315975a086","efbcd84bc4414aa4a1c16f51dd2f4400","bc267c2a7d774a39a061defb895c7f09","9f5b57925ee4447584d26bccc298c9eb","acd932183be745d3b521d327eb8a885a"]},"executionInfo":{"elapsed":57394,"status":"ok","timestamp":1705856573229,"user":{"displayName":"ahmed tarek","userId":"08412486906813382868"},"user_tz":-480},"id":"HrHUvmKw8iXU","outputId":"a955e6e1-c986-45f2-f8bd-8e7417ed819b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","24/24 [==============================] - 2s 57ms/step - loss: 0.7798 - accuracy: 0.5690 - val_loss: 0.6052 - val_accuracy: 0.7553\n","Epoch 2/20\n","24/24 [==============================] - 1s 50ms/step - loss: 0.5986 - accuracy: 0.6936 - val_loss: 0.5385 - val_accuracy: 0.7234\n","Epoch 3/20\n","24/24 [==============================] - 1s 49ms/step - loss: 0.5294 - accuracy: 0.7520 - val_loss: 0.6743 - val_accuracy: 0.5638\n","Epoch 4/20\n","24/24 [==============================] - 1s 59ms/step - loss: 0.4723 - accuracy: 0.7971 - val_loss: 0.5738 - val_accuracy: 0.7766\n","Epoch 5/20\n","24/24 [==============================] - 2s 87ms/step - loss: 0.4321 - accuracy: 0.8050 - val_loss: 0.4982 - val_accuracy: 0.7660\n","Epoch 6/20\n","24/24 [==============================] - 2s 85ms/step - loss: 0.3912 - accuracy: 0.8329 - val_loss: 0.6471 - val_accuracy: 0.6383\n","Epoch 7/20\n","24/24 [==============================] - 2s 78ms/step - loss: 0.3786 - accuracy: 0.8382 - val_loss: 0.6689 - val_accuracy: 0.7979\n","Epoch 8/20\n","24/24 [==============================] - 1s 47ms/step - loss: 0.3450 - accuracy: 0.8435 - val_loss: 0.4669 - val_accuracy: 0.8511\n","Epoch 9/20\n","24/24 [==============================] - 1s 48ms/step - loss: 0.2888 - accuracy: 0.8833 - val_loss: 0.4662 - val_accuracy: 0.7979\n","Epoch 10/20\n","24/24 [==============================] - 1s 49ms/step - loss: 0.3046 - accuracy: 0.8820 - val_loss: 0.4660 - val_accuracy: 0.8191\n","Epoch 11/20\n","24/24 [==============================] - 1s 48ms/step - loss: 0.2631 - accuracy: 0.8939 - val_loss: 0.6913 - val_accuracy: 0.8085\n","Epoch 12/20\n","24/24 [==============================] - 1s 49ms/step - loss: 0.2488 - accuracy: 0.8939 - val_loss: 0.4492 - val_accuracy: 0.8191\n","Epoch 13/20\n","24/24 [==============================] - 1s 49ms/step - loss: 0.2265 - accuracy: 0.9072 - val_loss: 0.5523 - val_accuracy: 0.8617\n","Epoch 14/20\n","24/24 [==============================] - 1s 48ms/step - loss: 0.2007 - accuracy: 0.9218 - val_loss: 0.4911 - val_accuracy: 0.8404\n","Epoch 15/20\n","24/24 [==============================] - 1s 49ms/step - loss: 0.2054 - accuracy: 0.9284 - val_loss: 0.4985 - val_accuracy: 0.8404\n","Epoch 16/20\n","24/24 [==============================] - 2s 72ms/step - loss: 0.1613 - accuracy: 0.9390 - val_loss: 0.6449 - val_accuracy: 0.8617\n","Epoch 17/20\n","24/24 [==============================] - 2s 85ms/step - loss: 0.1725 - accuracy: 0.9337 - val_loss: 0.6840 - val_accuracy: 0.7021\n","Epoch 18/20\n","24/24 [==============================] - 2s 83ms/step - loss: 0.1283 - accuracy: 0.9589 - val_loss: 1.5219 - val_accuracy: 0.7766\n","Epoch 19/20\n","24/24 [==============================] - 2s 69ms/step - loss: 0.1549 - accuracy: 0.9469 - val_loss: 0.5960 - val_accuracy: 0.8617\n","Epoch 20/20\n","24/24 [==============================] - 1s 48ms/step - loss: 0.1071 - accuracy: 0.9642 - val_loss: 0.7601 - val_accuracy: 0.8723\n","3/3 [==============================] - 0s 17ms/step - loss: 0.6570 - accuracy: 0.7895\n","Accuracy: 0.79\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54b62f6d7159459c8e3bc3315975a086","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Text(value='', description='URL:', placeholder='Enter Gun URL'),))"]},"metadata":{},"output_type":"display_data"}],"source":["# Constants\n","DATA_PATH = '/Datasets/WeaponDataset'\n","IMG_CATEGORIES = ['RealGun', 'NonRealGun']\n","\n","# Main\n","classifier = ImageClassifier(DATA_PATH, IMG_CATEGORIES)\n","data = classifier.load_images()\n","classifier.train_model(data)\n","\n","# User Input\n","def on_url_entered(change):\n","    classifier.predict_image(change.new)\n","\n","url_input = Text(placeholder='Enter Gun URL', description='URL:')\n","url_input.observe(on_url_entered, names='value')\n","display(VBox([url_input]))"]},{"cell_type":"markdown","metadata":{"id":"TgxExwXRAMig"},"source":["- Defines the constant `DATA_PATH` with the path to the dataset directory (`'/content/drive/MyDrive/WeaponDataset'`).\n","- Defines the constant `IMG_CATEGORIES` with a list of image categories (`['RealGun', 'NonRealGun']`).\n","\n","- Creates an instance of the `ImageClassifier` class named `classifier`, initialized with the dataset path and image categories.\n","  \n","- Loads images from the specified dataset path using the `load_images` method of the `classifier` object.\n","\n","- Trains the image classification model using the loaded data with the `train_model` method of the `classifier` object.\n","\n","- Defines a user input mechanism:\n","  - Creates a text input widget (`url_input`) with a placeholder and description.\n","  - Defines a function (`on_url_entered`) that is triggered when the user enters a URL. This function calls the `predict_image` method of the `classifier` object with the entered URL.\n","  - Associates the `on_url_entered` function with the `value` attribute of the `url_input` widget.\n","\n","- Displays the text input widget (`url_input`) inside a vertical box (`VBox`) using the `display` function.\n","\n","Note: Make sure to run this code in an environment where the `ImageClassifier` class is defined and the necessary libraries are imported. Also, the dataset path should be valid, and the dataset should contain images in the specified categories."]},{"cell_type":"markdown","metadata":{"id":"tzOnS9wYA4Ly"},"source":["# **Summary**"]},{"cell_type":"markdown","metadata":{"id":"EKsUqu7EBSzr"},"source":["The developed Python code provides a comprehensive solution for differentiating between images of real guns and water guns using a convolutional neural network (CNN). The process involves several key steps, including image loading, preprocessing, model training, and interactive user input handling for real-time classification.\n","\n","\n","* **Class Definition:** A custom class encapsulates the entire functionality. This class includes methods for loading images from a specified directory, preprocessing these images, training a CNN model on these images, and predicting the category of new images based on the trained model.\n","\n","* **Model Training:** The CNN model is designed with convolutional layers, pooling layers, and dense layers. It is trained on a dataset categorized into 'RealGun' and 'NonRealGun'. The training involves resizing images to a uniform size and normalizing them. The model learns features from these images to distinguish between the two categories.\n","\n","* **User Interaction:** An interactive component is built using ipywidgets. Users can input URLs of images, and the classifier predicts whether the image is a real gun or a water gun. This real-time interaction demonstrates the practical application of the mode"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPIFDVDuP7PZeyWpg1zyDPa","mount_file_id":"1ACeJRF2AwVlM0Ghq7Rs1wqDxrkODRoVu","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"54b62f6d7159459c8e3bc3315975a086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_efbcd84bc4414aa4a1c16f51dd2f4400"],"layout":"IPY_MODEL_bc267c2a7d774a39a061defb895c7f09"}},"9f5b57925ee4447584d26bccc298c9eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acd932183be745d3b521d327eb8a885a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc267c2a7d774a39a061defb895c7f09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efbcd84bc4414aa4a1c16f51dd2f4400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"TextModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"URL:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_9f5b57925ee4447584d26bccc298c9eb","placeholder":"Enter Gun URL","style":"IPY_MODEL_acd932183be745d3b521d327eb8a885a","value":""}}}}},"nbformat":4,"nbformat_minor":0}
